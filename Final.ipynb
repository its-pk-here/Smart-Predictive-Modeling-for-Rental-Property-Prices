{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h_wdCK-yBTcp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/dfs.csv\")"
      ],
      "metadata": {
        "id": "vVstpsxnBY8F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lazypredict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acdbPvYUBkYg",
        "outputId": "6e9b2697-680c-48a0-8a7f-8632367f71cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lazypredict\n",
            "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.66.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.3.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.1.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lazypredict) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->lazypredict) (1.16.0)\n",
            "Installing collected packages: lazypredict\n",
            "Successfully installed lazypredict-0.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lazypredict"
      ],
      "metadata": {
        "id": "dyJHdR4BBlZM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X= df.drop(\"rent\",axis=1)\n",
        "y=df[\"rent\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.3,random_state =123)"
      ],
      "metadata": {
        "id": "yy2lAf16Bn_s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "\n",
        "regressor = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "models, predictions = regressor.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Print the performance of each model\n",
        "print(models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28KtknyXBsVf",
        "outputId": "90bf7ed1-f663-41da-ef54-fcfaa2ab182f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [02:25<00:00,  3.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001477 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 923\n",
            "[LightGBM] [Info] Number of data points in the train set: 13600, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 19143.431029\n",
            "                               Adjusted R-Squared  R-Squared     RMSE  \\\n",
            "Model                                                                   \n",
            "LGBMRegressor                                0.82       0.82  3593.23   \n",
            "HistGradientBoostingRegressor                0.82       0.82  3599.29   \n",
            "XGBRegressor                                 0.81       0.81  3631.06   \n",
            "RandomForestRegressor                        0.80       0.80  3737.35   \n",
            "GradientBoostingRegressor                    0.78       0.78  3905.37   \n",
            "ExtraTreesRegressor                          0.78       0.78  3937.59   \n",
            "BaggingRegressor                             0.78       0.78  3956.45   \n",
            "LassoCV                                      0.66       0.66  4874.30   \n",
            "LassoLarsCV                                  0.66       0.66  4874.30   \n",
            "LarsCV                                       0.66       0.66  4874.30   \n",
            "LassoLarsIC                                  0.66       0.66  4874.59   \n",
            "LassoLars                                    0.66       0.66  4875.04   \n",
            "Lasso                                        0.66       0.66  4875.04   \n",
            "LinearRegression                             0.66       0.66  4875.13   \n",
            "TransformedTargetRegressor                   0.66       0.66  4875.13   \n",
            "Lars                                         0.66       0.66  4875.13   \n",
            "Ridge                                        0.66       0.66  4875.14   \n",
            "RidgeCV                                      0.66       0.66  4875.20   \n",
            "BayesianRidge                                0.66       0.66  4875.29   \n",
            "HuberRegressor                               0.66       0.66  4890.22   \n",
            "SGDRegressor                                 0.66       0.66  4892.89   \n",
            "PassiveAggressiveRegressor                   0.66       0.66  4917.05   \n",
            "KNeighborsRegressor                          0.65       0.65  4958.17   \n",
            "OrthogonalMatchingPursuitCV                  0.65       0.65  4974.95   \n",
            "ElasticNet                                   0.64       0.64  5046.90   \n",
            "TweedieRegressor                             0.61       0.61  5260.95   \n",
            "PoissonRegressor                             0.61       0.61  5262.98   \n",
            "DecisionTreeRegressor                        0.60       0.60  5307.06   \n",
            "GammaRegressor                               0.59       0.59  5357.67   \n",
            "OrthogonalMatchingPursuit                    0.55       0.55  5618.51   \n",
            "RANSACRegressor                              0.54       0.54  5699.62   \n",
            "AdaBoostRegressor                            0.50       0.51  5893.76   \n",
            "ExtraTreeRegressor                           0.50       0.50  5913.03   \n",
            "MLPRegressor                                 0.42       0.42  6362.46   \n",
            "ElasticNetCV                                 0.38       0.38  6619.72   \n",
            "NuSVR                                        0.06       0.06  8130.17   \n",
            "SVR                                          0.05       0.06  8144.38   \n",
            "DummyRegressor                              -0.00      -0.00  8390.24   \n",
            "LinearSVR                                   -0.30      -0.29  9540.22   \n",
            "KernelRidge                                 -4.57      -4.55 19761.63   \n",
            "GaussianProcessRegressor                    -9.82      -9.79 27552.41   \n",
            "\n",
            "                               Time Taken  \n",
            "Model                                      \n",
            "LGBMRegressor                        0.25  \n",
            "HistGradientBoostingRegressor        0.66  \n",
            "XGBRegressor                         0.24  \n",
            "RandomForestRegressor                6.25  \n",
            "GradientBoostingRegressor            2.06  \n",
            "ExtraTreesRegressor                  3.87  \n",
            "BaggingRegressor                     0.62  \n",
            "LassoCV                              0.19  \n",
            "LassoLarsCV                          0.07  \n",
            "LarsCV                               0.05  \n",
            "LassoLarsIC                          0.05  \n",
            "LassoLars                            0.04  \n",
            "Lasso                                0.02  \n",
            "LinearRegression                     0.02  \n",
            "TransformedTargetRegressor           0.02  \n",
            "Lars                                 0.02  \n",
            "Ridge                                0.01  \n",
            "RidgeCV                              0.03  \n",
            "BayesianRidge                        0.05  \n",
            "HuberRegressor                       0.07  \n",
            "SGDRegressor                         0.04  \n",
            "PassiveAggressiveRegressor           0.04  \n",
            "KNeighborsRegressor                  0.44  \n",
            "OrthogonalMatchingPursuitCV          0.03  \n",
            "ElasticNet                           0.02  \n",
            "TweedieRegressor                     0.02  \n",
            "PoissonRegressor                     0.24  \n",
            "DecisionTreeRegressor                0.18  \n",
            "GammaRegressor                       0.06  \n",
            "OrthogonalMatchingPursuit            0.02  \n",
            "RANSACRegressor                      0.17  \n",
            "AdaBoostRegressor                    0.89  \n",
            "ExtraTreeRegressor                   0.06  \n",
            "MLPRegressor                         7.19  \n",
            "ElasticNetCV                         0.18  \n",
            "NuSVR                                8.36  \n",
            "SVR                                 10.96  \n",
            "DummyRegressor                       0.01  \n",
            "LinearSVR                            0.03  \n",
            "KernelRidge                         22.95  \n",
            "GaussianProcessRegressor            79.09  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Using the best parameters from the previous LGBM model\n",
        "best_lgbm_params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'n_estimators': 300,\n",
        "    'num_leaves': 31\n",
        "}\n",
        "\n",
        "# Initialize LGBM with the best parameters and regularization\n",
        "model = lgb.LGBMRegressor(**best_lgbm_params, random_state=123, reg_alpha=0.1, reg_lambda=0.1)\n",
        "\n",
        "# Perform cross-validation to get the RMSE\n",
        "scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Calculate the average RMSE across all folds\n",
        "rmse_reg = sqrt(-scores.mean())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_reg = model.predict(X_test)\n",
        "\n",
        "# Calculate the RMSE on the test set\n",
        "rmse_test_reg = sqrt(mean_squared_error(y_test, y_pred_reg))\n",
        "\n",
        "# Output the RMSE from cross-validation and the test set\n",
        "print('Cross-validated RMSE with regularization:', rmse_reg)\n",
        "print('Test set RMSE with regularization:', rmse_test_reg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM3Miln9Buvf",
        "outputId": "9847bf55-10e6-41b4-de07-7028952fc854"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 902\n",
            "[LightGBM] [Info] Number of data points in the train set: 10880, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 19124.431342\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001279 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 903\n",
            "[LightGBM] [Info] Number of data points in the train set: 10880, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 19182.322610\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001272 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 902\n",
            "[LightGBM] [Info] Number of data points in the train set: 10880, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 19094.714522\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001290 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 904\n",
            "[LightGBM] [Info] Number of data points in the train set: 10880, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 19186.317371\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 900\n",
            "[LightGBM] [Info] Number of data points in the train set: 10880, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 19129.369301\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001159 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 909\n",
            "[LightGBM] [Info] Number of data points in the train set: 13600, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score 19143.431029\n",
            "Cross-validated RMSE with regularization: 3659.870703331728\n",
            "Test set RMSE with regularization: 3544.005176472708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2 = r2_score(y_test, y_pred_reg)\n",
        "\n",
        "print(f\"R-squared score: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVbG6Jy-By61",
        "outputId": "bc1ecba2-4f3c-4355-c036-48e53b9b39a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared score: 0.8215602140973817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "PPcgmvicB1Ve"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}